{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Bayesian Optimization with Random Forest\n",
    "\n",
    "## Optimizing a CNN with Scikit-Optimize\n",
    "In this note, we will use Bayesian Operation to select the best hyperparameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "\n",
    "np.random.seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import forest_minimize, gbrt_minimize\n",
    "from skopt.space import Real,Categorical, Integer\n",
    "from skopt.plots import plot_convergence, plot_objective, plot_evaluations\n",
    "from skopt.utils import use_named_args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Image re-scaling\n",
    "We re-scale data for the CNN, between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Reshape\n",
    "The images were stored in a pandas dataframe as 10d vectors of 784 vales. For a CNN with Keras, we need tensors with heigh x channel\n",
    "\n",
    "Thus, we reshape all data to 28 x 28 x 1, 3-D matrices\n",
    "\n",
    "The 3rd dimension corresponds to the channel. RGB imagens have 3 channels. Mnis images are in gray scale 2-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Target encoding\n",
    "np.unique(y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For keras, we need to create 10 dummy variables, one for each digiti\n",
    "\n",
    "#Encode labels to one hot vectors( ex : digit 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes= 10)\n",
    "y_test = to_categorical(y_test, num_classes= 10)\n",
    "\n",
    "y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAa9klEQVR4nO3df3DU953f8deaH2vgVnunYmlXQVZUB2oPoqQBwo/DIGhQ0Y0ZY5wctm8ykCYe/xDcUOH6gukUXSaHfOTMkIts0nhyGCYQmNxgTAtnrBxI2INxZQ7HlLhEPkRQDskqstkVMl6Q+PQPytYLWOSz3uWtlZ6PmZ1Bu9833w9ff+2nv+zqq4BzzgkAAAO3WS8AADB4ESEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGBmqPUCrnX58mWdOXNGoVBIgUDAejkAAE/OOXV1damoqEi33db3tU6/i9CZM2dUXFxsvQwAwOfU2tqqMWPG9LlNv4tQKBSSJM3Un2iohhmvBgDgq0eX9Ib2Jv973pesReiFF17QD37wA7W1tWn8+PHasGGD7r333pvOXf0ruKEapqEBIgQAOef/3ZH093lLJSsfTNixY4dWrFih1atX6+jRo7r33ntVWVmp06dPZ2N3AIAclZUIrV+/Xt/+9rf1ne98R/fcc482bNig4uJibdy4MRu7AwDkqIxH6OLFizpy5IgqKipSnq+oqNChQ4eu2z6RSCgej6c8AACDQ8YjdPbsWfX29qqwsDDl+cLCQrW3t1+3fW1trcLhcPLBJ+MAYPDI2jerXvuGlHPuhm9SrVq1SrFYLPlobW3N1pIAAP1Mxj8dN3r0aA0ZMuS6q56Ojo7rro4kKRgMKhgMZnoZAIAckPEroeHDh2vSpEmqr69Peb6+vl4zZszI9O4AADksK98nVF1drW9+85uaPHmypk+frp/85Cc6ffq0Hn/88WzsDgCQo7ISocWLF6uzs1Pf+9731NbWprKyMu3du1clJSXZ2B0AIEcFnHPOehGfFo/HFQ6HVa77uWMCAOSgHndJDXpFsVhMeXl5fW7Lj3IAAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzAy1XgDQnwSG+v8rMeSO0VlYSWaceOqLac31jrzsPVNyV4f3zMgnA94z7euHe8/80+Qd3jOSdLa323tm6i9Wes98qfqw98xAwZUQAMAMEQIAmMl4hGpqahQIBFIekUgk07sBAAwAWXlPaPz48frlL3+Z/HrIkCHZ2A0AIMdlJUJDhw7l6gcAcFNZeU+oublZRUVFKi0t1UMPPaSTJ09+5raJRELxeDzlAQAYHDIeoalTp2rLli3at2+fXnzxRbW3t2vGjBnq7Oy84fa1tbUKh8PJR3FxcaaXBADopzIeocrKSj344IOaMGGCvva1r2nPnj2SpM2bN99w+1WrVikWiyUfra2tmV4SAKCfyvo3q44aNUoTJkxQc3PzDV8PBoMKBoPZXgYAoB/K+vcJJRIJvffee4pGo9neFQAgx2Q8Qk899ZQaGxvV0tKit956S1//+tcVj8e1ZMmSTO8KAJDjMv7Xcb/73e/08MMP6+zZs7rjjjs0bdo0HT58WCUlJZneFQAgx2U8Qtu3b8/0b4l+asg9Y71nXHCY98yZ2X/oPXNhmv+NJyUpP+w/9/rE9G6OOdD8w8ch75m/rpvvPfPWhG3eMy2XLnjPSNKzH8zznil63aW1r8GKe8cBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGay/kPt0P/1ln8lrbn1Lz3vPTNu2PC09oVb65Lr9Z75rz9a6j0ztNv/Zp/Tf7HMeyb0Lz3eM5IUPOt/49ORb7+V1r4GK66EAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIa7aEPBE2fSmjvySbH3zLhhH6S1r4FmZds075mT50d7z7x01997z0hS7LL/3a0L//ZQWvvqz/yPAnxxJQQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmOEGplBPW3tacz/66294z/zV/G7vmSHv/oH3zK+e/JH3TLq+f/bfes+8/7WR3jO959q8Zx6Z/qT3jCSd+nP/mVL9Kq19YXDjSggAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMMMNTJG2/E1ves/c8d//lfdMb+eH3jPjy/6j94wkHZ/1d94zu38y23um4Nwh75l0BN5M76aipf7/aIG0cCUEADBDhAAAZrwjdPDgQS1YsEBFRUUKBALatWtXyuvOOdXU1KioqEgjRoxQeXm5jh8/nqn1AgAGEO8IdXd3a+LEiaqrq7vh6+vWrdP69etVV1enpqYmRSIRzZs3T11dXZ97sQCAgcX7gwmVlZWqrKy84WvOOW3YsEGrV6/WokWLJEmbN29WYWGhtm3bpscee+zzrRYAMKBk9D2hlpYWtbe3q6KiIvlcMBjU7NmzdejQjT8NlEgkFI/HUx4AgMEhoxFqb2+XJBUWFqY8X1hYmHztWrW1tQqHw8lHcXFxJpcEAOjHsvLpuEAgkPK1c+66565atWqVYrFY8tHa2pqNJQEA+qGMfrNqJBKRdOWKKBqNJp/v6Oi47uroqmAwqGAwmMllAAByREavhEpLSxWJRFRfX5987uLFi2psbNSMGTMyuSsAwADgfSV0/vx5vf/++8mvW1pa9M477yg/P1933nmnVqxYobVr12rs2LEaO3as1q5dq5EjR+qRRx7J6MIBALnPO0Jvv/225syZk/y6urpakrRkyRK99NJLevrpp3XhwgU9+eST+uijjzR16lS99tprCoVCmVs1AGBACDjnnPUiPi0ejyscDqtc92toYJj1cpCjfvPfpqQ3d9+PvWe+9dt/7z3zf2am8c3bl3v9ZwADPe6SGvSKYrGY8vLy+tyWe8cBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADATEZ/sirQX9zzF79Ja+5bE/zviL2p5B+9Z2Z/o8p7JrTjsPcM0N9xJQQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmOEGphiQes/F0prrfOIe75nTuy94z3z3+1u8Z1b96QPeM+5o2HtGkor/6k3/IefS2hcGN66EAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAz3MAU+JTLv3rPe+ahv/zP3jNb1/yN98w70/xveqpp/iOSNH7UMu+ZsS+2ec/0nDzlPYOBhSshAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMBMwDnnrBfxafF4XOFwWOW6X0MDw6yXA2SF++Mve8/kPfs775mf/+t93jPpuvvAd7xn/s1fxrxneptPes/g1upxl9SgVxSLxZSXl9fntlwJAQDMECEAgBnvCB08eFALFixQUVGRAoGAdu3alfL60qVLFQgEUh7TpqX5Q00AAAOad4S6u7s1ceJE1dXVfeY28+fPV1tbW/Kxd+/ez7VIAMDA5P2TVSsrK1VZWdnnNsFgUJFIJO1FAQAGh6y8J9TQ0KCCggKNGzdOjz76qDo6Oj5z20QioXg8nvIAAAwOGY9QZWWltm7dqv379+u5555TU1OT5s6dq0QiccPta2trFQ6Hk4/i4uJMLwkA0E95/3XczSxevDj567KyMk2ePFklJSXas2ePFi1adN32q1atUnV1dfLreDxOiABgkMh4hK4VjUZVUlKi5ubmG74eDAYVDAazvQwAQD+U9e8T6uzsVGtrq6LRaLZ3BQDIMd5XQufPn9f777+f/LqlpUXvvPOO8vPzlZ+fr5qaGj344IOKRqM6deqUnnnmGY0ePVoPPPBARhcOAMh93hF6++23NWfOnOTXV9/PWbJkiTZu3Khjx45py5YtOnfunKLRqObMmaMdO3YoFAplbtUAgAGBG5gCOWJIYYH3zJnFX0prX2/9xQ+9Z25L42/3/6ylwnsmNrPTewa3FjcwBQDkBCIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJjJ+k9WBZAZvR90eM8U/q3/jCR98nSP98zIwHDvmRe/+D+8Z+57YIX3zMiX3/Kewa3BlRAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYbmAIGLs/8svfMP3/jdu+Zsi+f8p6R0rsZaTp+9OG/854Z+crbWVgJrHAlBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCY4QamwKcEJpd5z/zmz/1v9vniH2/2npl1+0XvmVsp4S55zxz+sNR/R5fb/GfQb3ElBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCY4Qam6PeGlpZ4z/zzt4rS2lfN4u3eMw/+wdm09tWfPfPBZO+Zxh9O8575o81ves9gYOFKCABghggBAMx4Rai2tlZTpkxRKBRSQUGBFi5cqBMnTqRs45xTTU2NioqKNGLECJWXl+v48eMZXTQAYGDwilBjY6Oqqqp0+PBh1dfXq6enRxUVFeru7k5us27dOq1fv151dXVqampSJBLRvHnz1NXVlfHFAwBym9cHE1599dWUrzdt2qSCggIdOXJEs2bNknNOGzZs0OrVq7Vo0SJJ0ubNm1VYWKht27bpsccey9zKAQA573O9JxSLxSRJ+fn5kqSWlha1t7eroqIiuU0wGNTs2bN16NChG/4eiURC8Xg85QEAGBzSjpBzTtXV1Zo5c6bKysokSe3t7ZKkwsLClG0LCwuTr12rtrZW4XA4+SguLk53SQCAHJN2hJYtW6Z3331XP//5z697LRAIpHztnLvuuatWrVqlWCyWfLS2tqa7JABAjknrm1WXL1+u3bt36+DBgxozZkzy+UgkIunKFVE0Gk0+39HRcd3V0VXBYFDBYDCdZQAAcpzXlZBzTsuWLdPOnTu1f/9+lZaWprxeWlqqSCSi+vr65HMXL15UY2OjZsyYkZkVAwAGDK8roaqqKm3btk2vvPKKQqFQ8n2ecDisESNGKBAIaMWKFVq7dq3Gjh2rsWPHau3atRo5cqQeeeSRrPwBAAC5yytCGzdulCSVl5enPL9p0yYtXbpUkvT000/rwoULevLJJ/XRRx9p6tSpeu211xQKhTKyYADAwBFwzjnrRXxaPB5XOBxWue7X0MAw6+WgD0O/eKf3TGxS9OYbXWPx9169+UbXePwPT3rP9Hcr2/xvEPrmC/43IpWk/Jf+p//Q5d609oWBp8ddUoNeUSwWU15eXp/bcu84AIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmEnrJ6ui/xoajXjPfPh3o9La1xOljd4zD4c+SGtf/dmyf5npPfNPG7/sPTP67/+X90x+15veM8CtxJUQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGG5jeIhf/w2T/mf/0offMM1/a6z1TMaLbe6a/+6D3Qlpzs3av9J65+7/8b++Z/HP+Nxa97D0B9H9cCQEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZriB6S1yaqF/738z4RdZWEnmPH/uLu+ZHzZWeM8EegPeM3d/v8V7RpLGfvCW90xvWnsCIHElBAAwRIQAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYCTjnnPUiPi0ejyscDqtc92toYJj1cgAAnnrcJTXoFcViMeXl5fW5LVdCAAAzRAgAYMYrQrW1tZoyZYpCoZAKCgq0cOFCnThxImWbpUuXKhAIpDymTZuW0UUDAAYGrwg1NjaqqqpKhw8fVn19vXp6elRRUaHu7u6U7ebPn6+2trbkY+/evRldNABgYPD6yaqvvvpqytebNm1SQUGBjhw5olmzZiWfDwaDikQimVkhAGDA+lzvCcViMUlSfn5+yvMNDQ0qKCjQuHHj9Oijj6qjo+Mzf49EIqF4PJ7yAAAMDmlHyDmn6upqzZw5U2VlZcnnKysrtXXrVu3fv1/PPfecmpqaNHfuXCUSiRv+PrW1tQqHw8lHcXFxuksCAOSYtL9PqKqqSnv27NEbb7yhMWPGfOZ2bW1tKikp0fbt27Vo0aLrXk8kEimBisfjKi4u5vuEACBH+XyfkNd7QlctX75cu3fv1sGDB/sMkCRFo1GVlJSoubn5hq8Hg0EFg8F0lgEAyHFeEXLOafny5Xr55ZfV0NCg0tLSm850dnaqtbVV0Wg07UUCAAYmr/eEqqqq9LOf/Uzbtm1TKBRSe3u72tvbdeHCBUnS+fPn9dRTT+nNN9/UqVOn1NDQoAULFmj06NF64IEHsvIHAADkLq8roY0bN0qSysvLU57ftGmTli5dqiFDhujYsWPasmWLzp07p2g0qjlz5mjHjh0KhUIZWzQAYGDw/uu4vowYMUL79u37XAsCAAwe3DsOAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGBmqPUCruWckyT16JLkjBcDAPDWo0uS/v9/z/vS7yLU1dUlSXpDe41XAgD4PLq6uhQOh/vcJuB+n1TdQpcvX9aZM2cUCoUUCARSXovH4youLlZra6vy8vKMVmiP43AFx+EKjsMVHIcr+sNxcM6pq6tLRUVFuu22vt/16XdXQrfddpvGjBnT5zZ5eXmD+iS7iuNwBcfhCo7DFRyHK6yPw82ugK7igwkAADNECABgJqciFAwGtWbNGgWDQeulmOI4XMFxuILjcAXH4YpcOw797oMJAIDBI6euhAAAAwsRAgCYIUIAADNECABgJqci9MILL6i0tFS33367Jk2apNdff916SbdUTU2NAoFAyiMSiVgvK+sOHjyoBQsWqKioSIFAQLt27Up53TmnmpoaFRUVacSIESovL9fx48dtFptFNzsOS5cuve78mDZtms1is6S2tlZTpkxRKBRSQUGBFi5cqBMnTqRsMxjOh9/nOOTK+ZAzEdqxY4dWrFih1atX6+jRo7r33ntVWVmp06dPWy/tlho/frza2tqSj2PHjlkvKeu6u7s1ceJE1dXV3fD1devWaf369aqrq1NTU5MikYjmzZuXvA/hQHGz4yBJ8+fPTzk/9u4dWPdgbGxsVFVVlQ4fPqz6+nr19PSooqJC3d3dyW0Gw/nw+xwHKUfOB5cjvvrVr7rHH3885bm7777bffe73zVa0a23Zs0aN3HiROtlmJLkXn755eTXly9fdpFIxD377LPJ5z755BMXDofdj3/8Y4MV3hrXHgfnnFuyZIm7//77TdZjpaOjw0lyjY2NzrnBez5cexycy53zISeuhC5evKgjR46ooqIi5fmKigodOnTIaFU2mpubVVRUpNLSUj300EM6efKk9ZJMtbS0qL29PeXcCAaDmj179qA7NySpoaFBBQUFGjdunB599FF1dHRYLymrYrGYJCk/P1/S4D0frj0OV+XC+ZATETp79qx6e3tVWFiY8nxhYaHa29uNVnXrTZ06VVu2bNG+ffv04osvqr29XTNmzFBnZ6f10sxc/ec/2M8NSaqsrNTWrVu1f/9+Pffcc2pqatLcuXOVSCSsl5YVzjlVV1dr5syZKisrkzQ4z4cbHQcpd86HfncX7b5c+6MdnHPXPTeQVVZWJn89YcIETZ8+XXfddZc2b96s6upqw5XZG+znhiQtXrw4+euysjJNnjxZJSUl2rNnjxYtWmS4suxYtmyZ3n33Xb3xxhvXvTaYzofPOg65cj7kxJXQ6NGjNWTIkOv+T6ajo+O6/+MZTEaNGqUJEyaoubnZeilmrn46kHPjetFoVCUlJQPy/Fi+fLl2796tAwcOpPzol8F2PnzWcbiR/no+5ESEhg8frkmTJqm+vj7l+fr6es2YMcNoVfYSiYTee+89RaNR66WYKS0tVSQSSTk3Ll68qMbGxkF9bkhSZ2enWltbB9T54ZzTsmXLtHPnTu3fv1+lpaUprw+W8+Fmx+FG+u35YPihCC/bt293w4YNcz/96U/dr3/9a7dixQo3atQod+rUKeul3TIrV650DQ0N7uTJk+7w4cPuvvvuc6FQaMAfg66uLnf06FF39OhRJ8mtX7/eHT161P32t791zjn37LPPunA47Hbu3OmOHTvmHn74YReNRl08HjdeeWb1dRy6urrcypUr3aFDh1xLS4s7cOCAmz59uvvCF74woI7DE0884cLhsGtoaHBtbW3Jx8cff5zcZjCcDzc7Drl0PuRMhJxz7vnnn3clJSVu+PDh7itf+UrKxxEHg8WLF7toNOqGDRvmioqK3KJFi9zx48etl5V1Bw4ccJKueyxZssQ5d+VjuWvWrHGRSMQFg0E3a9Ysd+zYMdtFZ0Ffx+Hjjz92FRUV7o477nDDhg1zd955p1uyZIk7ffq09bIz6kZ/fklu06ZNyW0Gw/lws+OQS+cDP8oBAGAmJ94TAgAMTEQIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAmf8Lw4IYymq+HboAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SOme image examples\n",
    "g = plt.imshow(X_train[0][:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "#function to create the CNN\n",
    "def create_cnn(\n",
    "        # the hyperparamete to optimize are passed as argument\n",
    "        learning_rate, # Learning rate for the optimizer\n",
    "        num_conv_layers, # Number of conv layers\n",
    "        num_dense_layers, # number of dense layers\n",
    "        num_dense_nodes, # number of nodes in each dense layer\n",
    "        activation # Activation function for all layers\n",
    "):\n",
    "    model = Sequential()\n",
    "    \n",
    "    #We add the different number of conv laers in the following loop\n",
    "    for i in range(num_conv_layers):\n",
    "        model.add(Conv2D(kernel_size=5, strides=1, filters=16, padding='same', activation=activation))\n",
    "\n",
    "    model.add(MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "    #Second convolutional layer\n",
    "    # Same hyperparameter to optimize as previous layer\n",
    "    for i in range(num_conv_layers):\n",
    "        model.add(Conv2D(kernel_size=5, strides=1, filters=36, padding='same', activation=activation))\n",
    "    model.add(MaxPool2D(pool_size=2, strides=2))\n",
    "              \n",
    "    #Flatten the output of the last conv layer to pass it to the dense layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # We add the different number of dense layers in the following loop\n",
    "    for i in range(num_dense_layers):\n",
    "        model.add(Dense(num_dense_nodes, activation=activation))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the hyperparameter space\n",
    "\n",
    "- The number of fully-connected dense layers\n",
    "- The number of nodes(neurons) for each of the dense layers\n",
    "- Whether to use 'sigmoid' or 'relu' activation in all the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.utils import use_named_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_learningrate = Real(\n",
    "    low = 1e-6, high = 1e-2, prior='log-uniform', name =  'learning_rate',\n",
    ") \n",
    "dim_num_conv_layers = Integer(low=1, high=3, name='num_conv_layers')\n",
    "dim_num_dense_layers = Integer(low=1, high=3, name='num_dense_layers')\n",
    "dim_num_dense_nodes = Integer(low=5, high=512, name='num_dense_nodes')\n",
    "dim_activation = Categorical(categories=['relu', 'sigmoid'], name='activation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The hyperparameters space grid\n",
    "param_grid = [dim_learningrate,\n",
    "              dim_num_conv_layers,\n",
    "              dim_num_dense_layers,\n",
    "              dim_num_dense_nodes,\n",
    "              dim_activation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will save the model with this name\n",
    "path_best_model = 'cnn_model.h5'\n",
    "\n",
    "# Starting point for the optimization\n",
    "best_accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(param_grid)\n",
    "def objective(\n",
    "    learning_rate,\n",
    "    num_conv_layers,\n",
    "    num_dense_layers,\n",
    "    num_dense_nodes,\n",
    "    activation,\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:        Learning-rate for the optimizer.\n",
    "    convolutional layers: Number of conv layers.\n",
    "    num_dense_layers:     Number of dense layers.\n",
    "    num_dense_nodes:      Number of nodes in each dense layer.\n",
    "    activation:           Activation function for all layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Print the hyper-parameters.\n",
    "    print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "    print('num_conv_layers:', num_conv_layers)\n",
    "    print('num_dense_layers:', num_dense_layers)\n",
    "    print('num_dense_nodes:', num_dense_nodes)\n",
    "    print('activation:', activation)\n",
    "    print()\n",
    "    \n",
    "    # Create the neural network with the hyper-parameters.\n",
    "    # We call the function we created previously.\n",
    "    model = create_cnn(learning_rate=learning_rate,\n",
    "                       num_conv_layers=num_conv_layers,\n",
    "                       num_dense_layers=num_dense_layers,\n",
    "                       num_dense_nodes=num_dense_nodes,\n",
    "                       activation=activation)\n",
    "\n",
    "   \n",
    "    # Set a learning rate annealer\n",
    "    # this reduces the learning rate if learning does not improve\n",
    "    # for a certain number of epochs\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                                patience=2, \n",
    "                                                verbose=1, \n",
    "                                                factor=0.5, \n",
    "                                                min_lr=0.00001)\n",
    "   \n",
    "    # train the model\n",
    "    # we use 3 epochs to be able to run the notebook in a \"reasonable\"\n",
    "    # time. If we increase the epochs, we will have better performance\n",
    "    # this could be another parameter to optimize in fact.\n",
    "    history = model.fit(x=X_train,\n",
    "                        y=y_train,\n",
    "                        epochs=3,\n",
    "                        batch_size=128,\n",
    "                        validation_split=0.1,\n",
    "                        callbacks=learning_rate_reduction)\n",
    "\n",
    "    # Get the classification accuracy on the validation-set\n",
    "    # after the last training-epoch.\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print()\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "    print()\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_accuracy\n",
    "\n",
    "    # If the classification accuracy of the saved model is improved ...\n",
    "    if accuracy > best_accuracy:\n",
    "        # Save the new model to harddisk.\n",
    "        # Training CNNs is costly, so we want to avoid having to re-train\n",
    "        # the network with the best found parameters. We save it instead\n",
    "        # as we search for the best hyperparam space.\n",
    "        model.save(path_best_model)\n",
    "        \n",
    "        # Update the classification accuracy.\n",
    "        best_accuracy = accuracy\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "\n",
    "    \n",
    "    # Remember that Scikit-optimize always minimizes the objective\n",
    "    # function, so we need to negate the accuracy (because we want\n",
    "    # the maximum accuracy)\n",
    "    return -accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "objective() got an unexpected keyword argument 'learning_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 16\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Convert the list of default parameters to a dictionary\u001b[39;00m\n\u001b[0;32m      8\u001b[0m default_parameters_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      9\u001b[0m \t\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: default_parameters[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     10\u001b[0m \t\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_conv_layers\u001b[39m\u001b[38;5;124m'\u001b[39m: default_parameters[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \t\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m: default_parameters[\u001b[38;5;241m4\u001b[39m]\n\u001b[0;32m     14\u001b[0m }\n\u001b[1;32m---> 16\u001b[0m objective(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_parameters_dict)\n",
      "\u001b[1;31mTypeError\u001b[0m: objective() got an unexpected keyword argument 'learning_rate'"
     ]
    }
   ],
   "source": [
    "# Before we run the hyper-parameter optimization, \n",
    "# let's first check that the everything is working\n",
    "# by passing some default hyper-parameters.\n",
    "\n",
    "default_parameters = [1e-5, 1, 1, 16, 'relu']\n",
    "\n",
    "# Convert the list of default parameters to a dictionary\n",
    "default_parameters_dict = {\n",
    "\t'learning_rate': default_parameters[0],\n",
    "\t'num_conv_layers': default_parameters[1],\n",
    "\t'num_dense_layers': default_parameters[2],\n",
    "\t'num_dense_nodes': default_parameters[3],\n",
    "\t'activation': default_parameters[4]\n",
    "}\n",
    "\n",
    "objective(default_parameters_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 1e-05\n",
      "num_conv_layers: 1\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 16\n",
      "activation: relu\n"
     ]
    }
   ],
   "source": [
    "# Remova o decorador para testar\n",
    "def objective(learning_rate, num_conv_layers, num_dense_layers, num_dense_nodes, activation):\n",
    "    print(f\"learning_rate: {learning_rate}\")\n",
    "    print(f\"num_conv_layers: {num_conv_layers}\")\n",
    "    print(f\"num_dense_layers: {num_dense_layers}\")\n",
    "    print(f\"num_dense_nodes: {num_dense_nodes}\")\n",
    "    print(f\"activation: {activation}\")\n",
    "\n",
    "# Teste passando os parÃ¢metros diretamente\n",
    "objective(1e-5, 1, 1, 16, 'relu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Bayesian optimization with random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 1.0e-05\n",
      "num_conv_layers: 1\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 16\n",
      "activation: relu\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument(s) not recognized: {'lr': 1e-05}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#We could also approximate it with gradient boosting machines using gbrt_minimize\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m fm_ \u001b[38;5;241m=\u001b[39m forest_minimize(\n\u001b[0;32m      4\u001b[0m     objective, \u001b[38;5;66;03m# the objective function to minimize\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     param_grid, \u001b[38;5;66;03m# the hyperparameter space\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     x0 \u001b[38;5;241m=\u001b[39m default_parameters, \u001b[38;5;66;03m# the initial point\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     acq_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# The acquisition function\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     n_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m, \u001b[38;5;66;03m# the number of subsequent evaluations of f(x)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m     10\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vish8\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\forest.py:203\u001b[0m, in \u001b[0;36mforest_minimize\u001b[1;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, acq_func, initial_point_generator, x0, y0, random_state, verbose, callback, n_points, xi, kappa, n_jobs, model_queue_size, space_constraint)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforest_minimize\u001b[39m(\n\u001b[0;32m      7\u001b[0m     func,\n\u001b[0;32m      8\u001b[0m     dimensions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     space_constraint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     26\u001b[0m ):\n\u001b[0;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sequential optimisation using decision trees.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    A tree based regression model is used to model the expensive to evaluate\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m        :class:`skopt.dummy_minimize`, :class:`skopt.gbrt_minimize`\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m base_minimize(\n\u001b[0;32m    204\u001b[0m         func,\n\u001b[0;32m    205\u001b[0m         dimensions,\n\u001b[0;32m    206\u001b[0m         base_estimator,\n\u001b[0;32m    207\u001b[0m         n_calls\u001b[38;5;241m=\u001b[39mn_calls,\n\u001b[0;32m    208\u001b[0m         n_points\u001b[38;5;241m=\u001b[39mn_points,\n\u001b[0;32m    209\u001b[0m         n_random_starts\u001b[38;5;241m=\u001b[39mn_random_starts,\n\u001b[0;32m    210\u001b[0m         n_initial_points\u001b[38;5;241m=\u001b[39mn_initial_points,\n\u001b[0;32m    211\u001b[0m         initial_point_generator\u001b[38;5;241m=\u001b[39minitial_point_generator,\n\u001b[0;32m    212\u001b[0m         x0\u001b[38;5;241m=\u001b[39mx0,\n\u001b[0;32m    213\u001b[0m         y0\u001b[38;5;241m=\u001b[39my0,\n\u001b[0;32m    214\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mrandom_state,\n\u001b[0;32m    215\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    216\u001b[0m         acq_func\u001b[38;5;241m=\u001b[39macq_func,\n\u001b[0;32m    217\u001b[0m         xi\u001b[38;5;241m=\u001b[39mxi,\n\u001b[0;32m    218\u001b[0m         kappa\u001b[38;5;241m=\u001b[39mkappa,\n\u001b[0;32m    219\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    220\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    221\u001b[0m         acq_optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampling\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         space_constraint\u001b[38;5;241m=\u001b[39mspace_constraint,\n\u001b[0;32m    223\u001b[0m         model_queue_size\u001b[38;5;241m=\u001b[39mmodel_queue_size,\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\vish8\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\base.py:314\u001b[0m, in \u001b[0;36mbase_minimize\u001b[1;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size, space_constraint)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;66;03m# evaluate y0 if only x0 is provided\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x0 \u001b[38;5;129;01mand\u001b[39;00m y0 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 314\u001b[0m     y0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(func, x0))\n\u001b[0;32m    315\u001b[0m     n_calls \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y0)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# record through tell function\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vish8\\anaconda3\\Lib\\site-packages\\skopt\\utils.py:779\u001b[0m, in \u001b[0;36muse_named_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    776\u001b[0m arg_dict \u001b[38;5;241m=\u001b[39m {dim\u001b[38;5;241m.\u001b[39mname: value \u001b[38;5;28;01mfor\u001b[39;00m dim, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dimensions, x)}\n\u001b[0;32m    778\u001b[0m \u001b[38;5;66;03m# Call the wrapped objective function with the named arguments.\u001b[39;00m\n\u001b[1;32m--> 779\u001b[0m objective_value \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marg_dict)\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m objective_value\n",
      "Cell \u001b[1;32mIn[18], line 17\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(learning_rate, num_conv_layers, num_dense_layers, num_dense_nodes, activation)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#Create a neural network with the hyperparameters\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m create_cnn(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[0;32m     18\u001b[0m                    num_conv_layers\u001b[38;5;241m=\u001b[39mnum_conv_layers,\n\u001b[0;32m     19\u001b[0m                    num_dense_layers\u001b[38;5;241m=\u001b[39mnum_dense_layers,\n\u001b[0;32m     20\u001b[0m                    num_dense_nodes\u001b[38;5;241m=\u001b[39mnum_dense_nodes,\n\u001b[0;32m     21\u001b[0m                    activation\u001b[38;5;241m=\u001b[39mactivation)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#Set a learning rate annealer, this reduces the learning rate if learning odes not improve\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#for a certain number of epochs\u001b[39;00m\n\u001b[0;32m     25\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(x \u001b[38;5;241m=\u001b[39m X_train,\n\u001b[0;32m     26\u001b[0m                     y \u001b[38;5;241m=\u001b[39m y_train,\n\u001b[0;32m     27\u001b[0m                     epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     28\u001b[0m                     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m     29\u001b[0m                     validation_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     30\u001b[0m                     callbacks \u001b[38;5;241m=\u001b[39m learning_rate_reduction)\n",
      "Cell \u001b[1;32mIn[13], line 34\u001b[0m, in \u001b[0;36mcreate_cnn\u001b[1;34m(learning_rate, num_conv_layers, num_dense_layers, num_dense_nodes, activation)\u001b[0m\n\u001b[0;32m     31\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(Dense(num_dense_nodes, activation\u001b[38;5;241m=\u001b[39mactivation))\n\u001b[0;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m10\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 34\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     37\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     38\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\vish8\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\adam.py:62\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     45\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     61\u001b[0m ):\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     63\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[0;32m     64\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m     65\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m     66\u001b[0m         clipnorm\u001b[38;5;241m=\u001b[39mclipnorm,\n\u001b[0;32m     67\u001b[0m         clipvalue\u001b[38;5;241m=\u001b[39mclipvalue,\n\u001b[0;32m     68\u001b[0m         global_clipnorm\u001b[38;5;241m=\u001b[39mglobal_clipnorm,\n\u001b[0;32m     69\u001b[0m         use_ema\u001b[38;5;241m=\u001b[39muse_ema,\n\u001b[0;32m     70\u001b[0m         ema_momentum\u001b[38;5;241m=\u001b[39mema_momentum,\n\u001b[0;32m     71\u001b[0m         ema_overwrite_frequency\u001b[38;5;241m=\u001b[39mema_overwrite_frequency,\n\u001b[0;32m     72\u001b[0m         loss_scale_factor\u001b[38;5;241m=\u001b[39mloss_scale_factor,\n\u001b[0;32m     73\u001b[0m         gradient_accumulation_steps\u001b[38;5;241m=\u001b[39mgradient_accumulation_steps,\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     75\u001b[0m     )\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_1 \u001b[38;5;241m=\u001b[39m beta_1\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_2 \u001b[38;5;241m=\u001b[39m beta_2\n",
      "File \u001b[1;32mc:\\Users\\vish8\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py:21\u001b[0m, in \u001b[0;36mTFOptimizer.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n",
      "File \u001b[1;32mc:\\Users\\vish8\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:90\u001b[0m, in \u001b[0;36mBaseOptimizer.__init__\u001b[1;34m(self, learning_rate, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `decay` is no longer supported and will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     88\u001b[0m     )\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument(s) not recognized: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     name \u001b[38;5;241m=\u001b[39m auto_name(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Argument(s) not recognized: {'lr': 1e-05}"
     ]
    }
   ],
   "source": [
    "#We could also approximate it with gradient boosting machines using gbrt_minimize\n",
    "\n",
    "fm_ = forest_minimize(\n",
    "    objective, # the objective function to minimize\n",
    "    param_grid, # the hyperparameter space\n",
    "    x0 = default_parameters, # the initial point\n",
    "    acq_func = 'EI', # The acquisition function\n",
    "    n_calls = 30, # the number of subsequent evaluations of f(x)\n",
    "    random_state = 42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function value at the minimum.\n",
    "# note that it is the negative of the accuracy\n",
    "\n",
    "\"Best score=%.4f\" % fm_.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_.space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"Best parameters:\n",
    "=========================\n",
    "- learning rate=%.6f\n",
    "- num_conv_laayers=%d\n",
    "- num_dense_layers=%d\n",
    "- num_nodes=%d\n",
    "- activation = %s\"\"\" %(\n",
    "    fm_.x[0], \n",
    "    fm_.x[1],\n",
    "    fm_.x[2],\n",
    "    fm_.x[3],\n",
    "    fm_.x[4],\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(fm_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partially dependency plots\n",
    "\n",
    "[plot_objective](https://scikit-optimize.readthedocs.io/en/latest/modules/generated/skopt.plots.plot_objective.html#skopt.plots.plot_objective)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_names = ['learning_rate', 'num_conv_layers', 'num_dense_layers', 'num_dense_nodes', 'activation']\n",
    "plot_objective(result=fm_, plot_dims=dim_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "\n",
    "model = load_model(path_best_model)\n",
    "# make predictions in test set\n",
    "\n",
    "result = model.evaluate(x=X_test,\n",
    "                        y=y_test)\n",
    "\n",
    "# print evaluation metrics\n",
    "\n",
    "for name, value in zip(model.metrics_names, result):\n",
    "    print(name, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the values from the validation dataset\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predictions classes to one hot vectors \n",
    "y_pred_classes = np.argmax(y_pred, axis = 1)\n",
    "\n",
    "# Convert validation observations to one hot vectors\n",
    "y_true = np.argmax(y_test, axis = 1)\n",
    "\n",
    "# compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes) \n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make it more colourful\n",
    "classes = 10\n",
    "\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(classes)\n",
    "plt.xticks(tick_marks, range(classes), rotation=45)\n",
    "plt.yticks(tick_marks, range(classes))\n",
    "\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, cm[i, j],\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > 100 else \"black\",\n",
    "            )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
