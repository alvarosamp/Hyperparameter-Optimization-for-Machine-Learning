{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization of CNN with Scikit-Optimize\n",
    "\n",
    "In this notebook, we will use **Bayesian Optimization** to select the best **hyperparameters** for a CNN that recognizes digits in images, using the MNIST dataset and the open source Python package [Scikit-Optimize](https://scikit-optimize.readthedocs.io/en/latest).\n",
    "\n",
    "The MNIST dataset is availale in [Kaggle](https://www.kaggle.com/c/digit-recognizer/data).\n",
    "\n",
    "## Download dataset\n",
    "\n",
    "- Navigate to the [MNIST website in Kaggle](https://www.kaggle.com/c/digit-recognizer/data)\n",
    "- Download the train.csv file\n",
    "- Unzip and copy the train.csv file to where you see the SAVE_DATASETS-HERE.txt file\n",
    "- Rename to mnist.csv\n",
    "\n",
    "**Remember that you need to be logged in to be able to download the dataset**\n",
    "\n",
    "## Notebook content\n",
    "\n",
    "- Data Preparation\n",
    "- Set up a simple CNN\n",
    "- Set up the hyperparameter search shape\n",
    "- Set up the objective function\n",
    "- Perform Bayesian Optimization\n",
    "- Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cada iamgem tem 28x28 pixel, logo 784 pixels\n",
    "#The dataset has 785 columns. The first column contains the label (0-9) and the rest of the columns contain the pixel values (0-255).\n",
    "\n",
    "data = pd.read_csv(\"../mnist.csv\")\n",
    "\n",
    "# first column is the target, the rest of the columns\n",
    "# are the pixels of the image\n",
    "\n",
    "# each row is 1 image\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into a train and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(['label'], axis=1), # the images\n",
    "    data['label'], # the target\n",
    "    test_size = 0.1,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of images for each digit\n",
    "\n",
    "g = sns.countplot(x=y_train)\n",
    "plt.xlabel('Digits')\n",
    "plt.ylabel('Number of images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-scale the data\n",
    "\n",
    "# 255 is the maximum value a pixel can take\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape\n",
    "\n",
    "The images were stored in a pandas dataframe as 1-D vectors of 784 values. For a CNN with Keras, we need tensors with the following dimensions: width x height x channel. \n",
    "\n",
    "Thus, we reshape all data to 28 x 2 8 x 1, 3-D matrices. \n",
    "\n",
    "The 3rd dimension corresponds to the channel. RGB images have 3 channels. MNIST images are in gray-scale, thus they have only one channel in the 3rd dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape image in 3 dimensions:\n",
    "# height: 28px X width: 28px X channel: 1 \n",
    "\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "X_test = X_test.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Target encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Keras, we need to create 10 dummy variables,\n",
    "# one for each digit\n",
    "\n",
    "# Encode labels to one hot vectors (ex : digit 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "y_test = to_categorical(y_test, num_classes = 10)\n",
    "\n",
    "# the new target\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the CNN\n",
    "\n",
    "We will create a CNN, with 2 Convolutional layers followed by Pooling, and varying number of fully-connected Dense layers.\n",
    "\n",
    "In fact, the number of fully-connected Dense layers and the number of Neurons in each one of the Dense layers, are some of the hyperparameters we want to optimize.\n",
    "\n",
    "We could also optimize the number of Convolutional layers. But we will keep that for later, and here we keep it a bit simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create the CNN\n",
    "\n",
    "def create_cnn(\n",
    "    learning_rate,\n",
    "    num_dense_layers,\n",
    "    num_dense_nodes,\n",
    "    activation,\n",
    "):\n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:     Learning-rate for the optimizer.\n",
    "    num_dense_layers:  Number of dense layers.\n",
    "    num_dense_nodes:   Number of nodes in each dense layer.\n",
    "    activation:        Activation function for all layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Start construction of a Keras Sequential model.\n",
    "    model = Sequential()\n",
    "\n",
    "    # First convolutional layer.\n",
    "    # There are many hyper-parameters in this layer\n",
    "    # For this demo, we will optimize the activation function only.\n",
    "    model.add(Conv2D(kernel_size=5, strides=1, filters=16, padding='same',\n",
    "                     activation=activation, name='layer_conv1'))\n",
    "    model.add(MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "    # Second convolutional layer.\n",
    "    # Again, we will only optimize the activation function.\n",
    "    model.add(Conv2D(kernel_size=5, strides=1, filters=36, padding='same',\n",
    "                     activation=activation, name='layer_conv2'))\n",
    "    model.add(MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "    # Flatten the 4-rank output of the convolutional layers\n",
    "    # to 2-rank that can be input to a fully-connected Dense layer.\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Add fully-connected Dense layers.\n",
    "    # The number of layers is a hyper-parameter we want to optimize.\n",
    "    # We add the different number of layers in the following loop:\n",
    "    \n",
    "    for i in range(num_dense_layers):\n",
    "        \n",
    "        # Add the dense fully-connected layer to the model.\n",
    "        # This has two hyper-parameters we want to optimize:\n",
    "        # The number of nodes (neurons) and the activation function.\n",
    "        model.add(Dense(num_dense_nodes,\n",
    "                        activation=activation,\n",
    "                        ))\n",
    "\n",
    "    # Last fully-connected dense layer with softmax-activation\n",
    "    # for use in classification.\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # Use the Adam method for training the network.\n",
    "    # We want to find the best learning-rate for the Adam method.\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # In Keras we need to compile the model so it can be trained.\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Hyperparameter Space\n",
    "\n",
    "\n",
    "We want to find the following hyper-parameters:\n",
    "\n",
    "- The learning rate of the optimizer.\n",
    "- The number of fully-connected Dense layers.\n",
    "- The number of nodes (neurons) for each of the dense layers.\n",
    "- Whether to use 'sigmoid' or 'relu' activation in all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_learning_rate = Real(\n",
    "    low=1e-6, high=1e-2, prior='log-uniform', name='learning_rate',\n",
    ")\n",
    "\n",
    "dim_num_dense_layers = Integer(low=1, high=5, name='num_dense_layers')\n",
    "\n",
    "dim_num_dense_nodes = Integer(low=5, high=512, name='num_dense_nodes')\n",
    "\n",
    "\n",
    "dim_activation = Categorical(\n",
    "    categories=['relu', 'sigmoid'], name='activation',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the hyperparameter space grid\n",
    "\n",
    "param_grid = [dim_learning_rate,\n",
    "              dim_num_dense_layers,\n",
    "              dim_num_dense_nodes,\n",
    "              dim_activation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_best_model = 'cnn_model.keras'\n",
    "\n",
    "# starting point for the optimization\n",
    "best_accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(param_grid)\n",
    "def objective(\n",
    "    learning_rate,\n",
    "    num_dense_layers,\n",
    "    num_dense_nodes,\n",
    "    activation,\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:     Learning-rate for the optimizer.\n",
    "    num_dense_layers:  Number of dense layers.\n",
    "    num_dense_nodes:   Number of nodes in each dense layer.\n",
    "    activation:        Activation function for all layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Print the hyper-parameters.\n",
    "    print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "    print('num_dense_layers:', num_dense_layers)\n",
    "    print('num_dense_nodes:', num_dense_nodes)\n",
    "    print('activation:', activation)\n",
    "    print()\n",
    "    \n",
    "    # Create the neural network with the hyper-parameters.\n",
    "    # We call the function we created previously.\n",
    "    model = create_cnn(learning_rate=learning_rate,\n",
    "                       num_dense_layers=num_dense_layers,\n",
    "                       num_dense_nodes=num_dense_nodes,\n",
    "                       activation=activation)\n",
    "\n",
    "   \n",
    "    # Set a learning rate annealer\n",
    "    # this reduces the learning rate if learning does not improve\n",
    "    # for a certain number of epochs\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                                patience=2, \n",
    "                                                verbose=1, \n",
    "                                                factor=0.5, \n",
    "                                                min_lr=0.00001)\n",
    "   \n",
    "    # train the model\n",
    "    # we use 3 epochs to be able to run the notebook in a \"reasonable\"\n",
    "    # time. If we increase the epochs, we will have better performance\n",
    "    # this could be another parameter to optimize in fact.\n",
    "    history = model.fit(x=X_train,\n",
    "                        y=y_train,\n",
    "                        epochs=3,\n",
    "                        batch_size=128,\n",
    "                        validation_split=0.1,\n",
    "                        callbacks=learning_rate_reduction)\n",
    "\n",
    "    # Get the classification accuracy on the validation-set\n",
    "    # after the last training-epoch.\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print()\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "    print()\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_accuracy\n",
    "\n",
    "    # If the classification accuracy of the saved model is improved ...\n",
    "    if accuracy > best_accuracy:\n",
    "        # Save the new model to harddisk.\n",
    "        # Training CNNs is costly, so we want to avoid having to re-train\n",
    "        # the network with the best found parameters. We save it instead\n",
    "        # as we search for the best hyperparam space.\n",
    "        model.save(path_best_model)\n",
    "        \n",
    "        # Update the classification accuracy.\n",
    "        best_accuracy = accuracy\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "\n",
    "    \n",
    "    # Remember that Scikit-optimize always minimizes the objective\n",
    "    # function, so we need to negate the accuracy (because we want\n",
    "    # the maximum accuracy)\n",
    "    return -accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we run the hyper-parameter optimization, \n",
    "# let's first check that the everything is working\n",
    "# by passing some default hyper-parameters.\n",
    "\n",
    "default_parameters = [1e-5, 1, 16, 'relu']\n",
    "\n",
    "objective(x=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_ = gp_minimize(\n",
    "    objective, # the objective function to minimize\n",
    "    param_grid, # the hyperparameter space\n",
    "    x0=default_parameters, # the initial parameters to test\n",
    "    acq_func='EI', # the acquisition function\n",
    "    n_calls=30, # the number of subsequent evaluations of f(x)\n",
    "    random_state=0, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Best score=%.4f\" % gp_.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"Best parameters:\n",
    "=========================\n",
    "- learning rate=%.6f\n",
    "- num_dense_layers=%d\n",
    "- num_nodes=%d\n",
    "- activation = %s\"\"\" % (gp_.x[0], \n",
    "                gp_.x[1],\n",
    "                gp_.x[2],\n",
    "                gp_.x[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(gp_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "\n",
    "model = load_model(path_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions in test set\n",
    "\n",
    "result = model.evaluate(x=X_test,\n",
    "                        y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print evaluation metrics\n",
    "\n",
    "for name, value in zip(model.metrics_names, result):\n",
    "    print(name, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the values from the validation dataset\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predictions classes to one hot vectors \n",
    "y_pred_classes = np.argmax(y_pred, axis = 1)\n",
    "\n",
    "# Convert validation observations to one hot vectors\n",
    "y_true = np.argmax(y_test, axis = 1)\n",
    "\n",
    "# compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes) \n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make it more colourful\n",
    "classes = 10\n",
    "\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(classes)\n",
    "plt.xticks(tick_marks, range(classes), rotation=45)\n",
    "plt.yticks(tick_marks, range(classes))\n",
    "\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, cm[i, j],\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > 100 else \"black\",\n",
    "            )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
